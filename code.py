# -*- coding: utf-8 -*-
"""HOMEWORK 3 FINALE SIIII

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1h7ji3YLQgj2jgzsZ72VBdKJnyqzC6TbG

**Install requirements**
"""

!pip3 install 'torch==1.3.1'
!pip3 install 'torchvision==0.5.0'
!pip3 install 'Pillow-SIMD'
!pip3 install 'tqdm'
!pip3 install 'utils'

"""**Import libraries**"""

import os
import logging

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Subset, DataLoader
from torch.backends import cudnn
from torch.autograd import Function
from torchvision.models.utils import load_state_dict_from_url

import torchvision
from torchvision.datasets import ImageFolder
from torchvision import transforms

from PIL import Image
from tqdm import tqdm

import numpy as np

from sklearn.model_selection import ParameterGrid

import matplotlib.pyplot as plt

"""**READ DOMAIN**"""

# Clone github repository with data
if not os.path.isdir('./Homework3-PACS'):
  !git clone https://github.com/MachineLearning2020/Homework3-PACS.git

transformation = transforms.Compose([ transforms.Resize(230),      
                                      transforms.CenterCrop(224),  
                                      transforms.ToTensor(), 
                                      transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)) # Normalizes tensor with mean and standard deviation of IMAGENET                                    
])

art_painting_dataset=ImageFolder('Homework3-PACS/PACS/art_painting',transform=transformation)
cartoon_dataset=ImageFolder('Homework3-PACS/PACS/cartoon',transform=transformation)
photo_dataset=ImageFolder('Homework3-PACS/PACS/photo',transform=transformation)
sketch_dataset=ImageFolder('Homework3-PACS/PACS/sketch',transform=transformation)

def conta(data):
  lista=[]
  for i in range (len(data)):
    lista.append(data[i][1])
  classi=set(lista)
  occorrenze=[]
  for classe in classi:
    occorrenze.append(lista.count(classe))
  return occorrenze

occorrenze_art=conta(art_painting_dataset)
occorrenze_cartoon=conta(cartoon_dataset)
occorrenze_photo=conta(photo_dataset)
occorrenze_sketch=conta(sketch_dataset)

x = np.arange(len(classi))
width = 0.80 

fig, ax = plt.subplots(figsize=(10,4))
rects1 = ax.bar(x-width/8-width/4,occorrenze_art, 0.2, label='Art Painting')
rects2 = ax.bar( x-width/8, occorrenze_cartoon, 0.2, label='Cartoon')
rects3 = ax.bar(x+width/8, occorrenze_photo, 0.2, label='Photo')
rects4 = ax.bar(x+width/8+width/4, occorrenze_sketch, 0.2, label='Sketch')

ax.set_ylabel('Occurrences')
ax.set_title('Occurrences in the dataset')
ax.set_xticks(x)
ax.set_xticklabels(["dog","elephant","giraffe","guitar","horse","house","person"])
ax.legend()

fig.tight_layout()

plt.savefig("Data Exploration")

plt.show()

"""**MODIFIED ALEXNET**"""

__all__ = ['AlexNet', 'alexnet']
model_urls = {
    'alexnet': 'https://download.pytorch.org/models/alexnet-owt-4df8aa71.pth',
}


class ReverseLayerF(Function):
  
  @staticmethod
  def forward(ctx,x,alpha):
    ctx.alpha=alpha
    return x.view_as(x)
  
  @staticmethod
  def backward(ctx,grad_output):
    output=grad_output.neg()*ctx.alpha
    return output,None

class AlexNet_nuova(nn.Module):

    def __init__(self, num_classes=1000):
        super(AlexNet_nuova, self).__init__()
        self.features = nn.Sequential(
            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=3, stride=2),
            nn.Conv2d(64, 192, kernel_size=5, padding=2),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=3, stride=2),
            nn.Conv2d(192, 384, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(384, 256, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=3, stride=2),
        )
        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))
        self.classifier = nn.Sequential(
            nn.Dropout(),
            nn.Linear(256 * 6 * 6, 4096),
            nn.ReLU(inplace=True),
            nn.Dropout(),
            nn.Linear(4096, 4096),
            nn.ReLU(inplace=True),
            nn.Linear(4096, num_classes),
        )
        self.DANN_classifier = nn.Sequential(
            nn.Dropout(),
            nn.Linear(256 * 6 * 6, 4096),
            nn.ReLU(inplace=True),
            nn.Dropout(),
            nn.Linear(4096, 4096),
            nn.ReLU(inplace=True),
            nn.Linear(4096, 2),
        )

    def forward(self, x, alpha=None):
        x = self.features(x)
        x = self.avgpool(x)
        x = torch.flatten(x, 1)
        #we are training the discriminator
        if alpha is not None:
          reverse_feature=ReverseLayerF.apply(x,alpha)
          x=self.DANN_classifier(reverse_feature)
          return x
        #we are training with supervision
        else:
          x = self.classifier(x)
          return x


def alexnet_nuova(pretrained=False, progress=True, **kwargs):
    model = AlexNet_nuova(**kwargs)
    if pretrained:
        state_dict = load_state_dict_from_url(model_urls['alexnet'],progress=progress)
        model.load_state_dict(state_dict,strict=False)
    return model

"""**Set Arguments**"""

#PARAMETRI FISSI
DEVICE = 'cuda'    

NUM_CLASSES = 7     

MOMENTUM = 0.9      
WEIGHT_DECAY = 5e-5

"""**WITHOUT ADAPTATION**

*Model definition*
"""

def create_model(BATCH_SIZE,LR,NUM_EPOCHS,STEP_SIZE,GAMMA,SGD):
  flag=0
  if SGD==False:
    if LR>1e-4:
      return 0
    else:
      flag=1
  if SGD==True:
    if LR<1e-4:
      return 0
    else:
      flag=1
  if BATCH_SIZE>256:
    if LR<1e-3:
      return 0
    else:
      flag=1
  if flag==1:
    #DEFINE DATALOADER
    train_dataloader = DataLoader(photo_dataset, batch_size=BATCH_SIZE, shuffle=True,num_workers=4,drop_last=True)
    test_dataloader_cartoon = DataLoader(cartoon_dataset, batch_size=BATCH_SIZE, num_workers=4,shuffle=False)
    test_dataloader_sketch = DataLoader(sketch_dataset, batch_size=BATCH_SIZE,num_workers=4, shuffle=False)  
    #CREATE NET
    net = torchvision.models.alexnet(pretrained=True)
    net.classifier[6] = nn.Linear(4096, NUM_CLASSES)
    #TRAINING PREPARATION
    criterion = nn.CrossEntropyLoss() 
    parameters_to_optimize = net.parameters()
    if SGD==True: 
      optimizer = optim.SGD(parameters_to_optimize, lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)
    else:
      optimizer = optim.Adam(parameters_to_optimize,lr=LR)
    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=STEP_SIZE, gamma=GAMMA)
    #TRAINING DEFINITION
    net = net.to(DEVICE) 
    cudnn.benchmark
    current_step = 0
    for epoch in range(NUM_EPOCHS):
      for images, labels in train_dataloader:
        images = images.to(DEVICE)
        labels = labels.to(DEVICE)
        net.train()
        optimizer.zero_grad()
        outputs = net(images)
        loss = criterion(outputs, labels)
        loss.backward()  
        optimizer.step()
        current_step += 1
      scheduler.step() 

    net = net.to(DEVICE)
    net.train(False)
    #TESTING 1
    running_corrects = 0
    for images, labels in tqdm(test_dataloader_cartoon):
      images = images.to(DEVICE)
      labels = labels.to(DEVICE)
      outputs = net(images)
      _, preds = torch.max(outputs.data, 1)
      running_corrects += torch.sum(preds == labels.data).data.item()
    accuracy_cartoon = running_corrects / float(len(cartoon_dataset))
    print('Accuracy on cartoon: {}'.format(accuracy_cartoon))
    #TESTING 2
    running_corrects = 0
    for images, labels in tqdm(test_dataloader_sketch):
      images = images.to(DEVICE)
      labels = labels.to(DEVICE)
      outputs = net(images)
      _, preds = torch.max(outputs.data, 1)
      running_corrects += torch.sum(preds == labels.data).data.item()
    accuracy_sketch = running_corrects / float(len(sketch_dataset))
    print('Accuracy on sketch: {}'.format(accuracy_sketch))
    accuracy_finale=(accuracy_cartoon+accuracy_sketch)/2
    print(accuracy_finale)
    return accuracy_finale

"""*GRID PREPARATION*"""

#definizione dei range
BATCH_SIZE_range=[256,320]
LR_range=[1e-2,1e-3,1e-4,5e-5]
NUM_EPOCHS_range=[30,40]
STEP_SIZE_range=[15,20]
GAMMA_range=[0.1,0.2]
SGD_range=[True,False]
#preparazione della griglia
param_grid=dict(BATCH_SIZE=BATCH_SIZE_range,LR=LR_range,NUM_EPOCHS=NUM_EPOCHS_range,STEP_SIZE=STEP_SIZE_range,GAMMA=GAMMA_range,SGD=SGD_range)

best_acc=0
best_param=None
totale=len(list(ParameterGrid(param_grid)))
i=1
for combinazione in list(ParameterGrid(param_grid)):
  print(combinazione)
  print(f"Combination number {i} of {totale}")
  accuracy=create_model(combinazione["BATCH_SIZE"],combinazione["LR"],combinazione["NUM_EPOCHS"],combinazione["STEP_SIZE"],combinazione["GAMMA"],combinazione["SGD"])
  if accuracy>best_acc:
    best_acc=accuracy
    best_param=combinazione
  i=i+1

"""*BEST FOUND*"""

best_param
'''{'BATCH_SIZE': 256,
 'GAMMA': 0.1,
 'LR': 0.001,
 'NUM_EPOCHS': 30,
 'SGD': True,
 'STEP_SIZE': 20}
 '''

BATCH_SIZE = best_param["BATCH_SIZE"]
LR = best_param["LR"]            
NUM_EPOCHS = best_param["NUM_EPOCHS"]
STEP_SIZE = best_param["STEP_SIZE"]
GAMMA = best_param["GAMMA"]
SGD=best_param["SGD"]

train_dataloader = DataLoader(photo_dataset, batch_size=BATCH_SIZE, shuffle=True,num_workers=4,drop_last=True)
test_dataloader_art = DataLoader(art_painting_dataset, batch_size=BATCH_SIZE, num_workers=4,shuffle=False)
 
net = torchvision.models.alexnet(pretrained=True)
net.classifier[6] = nn.Linear(4096, NUM_CLASSES)

criterion = nn.CrossEntropyLoss() 
parameters_to_optimize = net.parameters() 
if SGD==True:
  optimizer = optim.SGD(parameters_to_optimize, lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)
else:
  optimizer = optim.Adam(parameters_to_optimize, lr=LR)
scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=STEP_SIZE, gamma=GAMMA)

"""*Train*"""

net = net.to(DEVICE) 
cudnn.benchmark

current_step = 0
for epoch in range(NUM_EPOCHS):
  print('Starting epoch {}/{}, LR = {}'.format(epoch+1, NUM_EPOCHS, scheduler.get_lr()))

  for images, labels in train_dataloader:
    images = images.to(DEVICE)
    labels = labels.to(DEVICE)

    net.train()
    optimizer.zero_grad()

    # Forward pass to the network
    outputs = net(images)

    loss = criterion(outputs, labels)

    # Compute gradients for each layer and update weights
    loss.backward()  # backward pass: computes gradients
    optimizer.step() # update weights based on accumulated gradients

    current_step += 1

  # Step the scheduler
  scheduler.step()

"""*Text*"""

net = net.to(DEVICE)
net.train(False)

running_corrects = 0
for images, labels in tqdm(test_dataloader_art):
  images = images.to(DEVICE)
  labels = labels.to(DEVICE)

  outputs = net(images)

  _, preds = torch.max(outputs.data, 1)

  running_corrects += torch.sum(preds == labels.data).data.item()

accuracy = running_corrects / float(len(art_painting_dataset))

print('Test Accuracy: {}'.format(accuracy))

accuracy_totale=[]
for i in range (5):
  BATCH_SIZE = best_param["BATCH_SIZE"]
  LR = best_param["LR"]           
  NUM_EPOCHS = best_param["NUM_EPOCHS"]
  STEP_SIZE = best_param["STEP_SIZE"]
  GAMMA = best_param["GAMMA"]
  SGD=best_param["SGD"]

  train_dataloader = DataLoader(photo_dataset, batch_size=BATCH_SIZE, shuffle=True,num_workers=4,drop_last=True)
  test_dataloader_art = DataLoader(art_painting_dataset, batch_size=BATCH_SIZE, num_workers=4,shuffle=False)
  
  net = torchvision.models.alexnet(pretrained=True)
  net.classifier[6] = nn.Linear(4096, NUM_CLASSES)

  criterion = nn.CrossEntropyLoss() 
  parameters_to_optimize = net.parameters() 
  if SGD==True:
    optimizer = optim.SGD(parameters_to_optimize, lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)
  else:
    optimizer = optim.Adam(parameters_to_optimize, lr=LR)
  scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=STEP_SIZE, gamma=GAMMA)


  net = net.to(DEVICE) 
  cudnn.benchmark

  current_step = 0
  for epoch in range(NUM_EPOCHS):
    print('Starting epoch {}/{}, LR = {}'.format(epoch+1, NUM_EPOCHS, scheduler.get_lr()))

    for images, labels in train_dataloader:
      images = images.to(DEVICE)
      labels = labels.to(DEVICE)

      net.train()
      optimizer.zero_grad()

      # Forward pass to the network
      outputs = net(images)

      loss = criterion(outputs, labels)

      # Compute gradients for each layer and update weights
      loss.backward()  # backward pass: computes gradients
      optimizer.step() # update weights based on accumulated gradients

      current_step += 1

    # Step the scheduler
    scheduler.step() 

  net = net.to(DEVICE)
  net.train(False)

  running_corrects = 0
  for images, labels in tqdm(test_dataloader_art):
    images = images.to(DEVICE)
    labels = labels.to(DEVICE)

    outputs = net(images)

    _, preds = torch.max(outputs.data, 1)

    running_corrects += torch.sum(preds == labels.data).data.item()

  accuracy = running_corrects / float(len(art_painting_dataset))
  accuracy_totale.append(accuracy)
  print('Test Accuracy: {}'.format(accuracy))

acc=np.array(accuracy_totale)
print(f"ACCURACY: {acc.mean()} +- {acc.std()}")

"""**WITH ADAPTATION**

*Model definition*
"""

def preparation_part(source_dataset,target_dataset,BATCH_SIZE,NUM_CLASSES,LR,GAMMA,SGD,STEP_SIZE):
  #DEFINE DATALOADER
  source_dataloder = DataLoader(source_dataset, batch_size=BATCH_SIZE, shuffle=True,num_workers=4,drop_last=True)
  target_dataloader=DataLoader(target_dataset, batch_size=BATCH_SIZE, num_workers=4,shuffle=True,drop_last=True)
  test_dataloader = DataLoader(target_dataset, batch_size=BATCH_SIZE, num_workers=4,shuffle=False)
  #CREATE NET
  nuova=alexnet_nuova(pretrained=True)
  nuova.DANN_classifier[1].weight.data=nuova.classifier[1].weight.data
  nuova.DANN_classifier[1].bias.data=nuova.classifier[1].bias.data
  nuova.DANN_classifier[4].weight.data=nuova.classifier[4].weight.data
  nuova.DANN_classifier[4].bias.data=nuova.classifier[4].bias.data
  nuova.DANN_classifier[6].weight.data=nuova.classifier[6].weight.data
  nuova.DANN_classifier[6].bias.data=nuova.classifier[6].bias.data
  nuova.classifier[6] = nn.Linear(4096, NUM_CLASSES)
  nuova.DANN_classifier[6] = nn.Linear(4096, 2)
  #TRAINING PREPARATION
  criterion = nn.CrossEntropyLoss() 
  parameters_to_optimize = nuova.parameters()
  if SGD==True: 
    optimizer = optim.SGD(parameters_to_optimize, lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)
  else:
    optimizer = optim.Adam(parameters_to_optimize,lr=LR)
  scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=STEP_SIZE, gamma=GAMMA)
  return nuova,source_dataloder,target_dataloader,test_dataloader,criterion,optimizer,scheduler

#def training_part(nuova,NUM_EPOCHS,BATCH_SIZE,ALFA,source_dataloder,target_dataloader,optimizer,criterion,scheduler):
def training_part(nuova,NUM_EPOCHS,BATCH_SIZE,LAMBDA,source_dataloder,target_dataloader,optimizer,criterion,scheduler):

  nuova = nuova.to(DEVICE) 
  cudnn.benchmark
  current_step = 0
  
  for epoch in range(NUM_EPOCHS):
    iteratore1=iter(source_dataloder)
    iteratore2=iter(target_dataloader)
    lunghezza=max(len(source_dataloder),len(target_dataloader))
    
    for i in range(lunghezza):
      try:
        data_source,label_source=iteratore1.next()
      except StopIteration:
        iteratore1=iter(source_dataloder)
        data_source,label_source=iteratore1.next()
      
      images = data_source.to(DEVICE)
      labels = label_source.to(DEVICE)
      nuova.train()
      optimizer.zero_grad()
      
      #1ST STEP
      outputs = nuova(images)
      loss = criterion(outputs, labels)
      loss.backward()
    
      #2ND STEP
      p = float(current_step + epoch * len(source_dataloder)) / NUM_EPOCHS / len(source_dataloder)
      ALFA = 2. / (1. + np.exp(-LAMBDA * p)) - 1
      outputs = nuova(images,ALFA)
      tensore=torch.tensor(np.zeros((BATCH_SIZE,), dtype=int)).to(DEVICE)
      loss = criterion(outputs,tensore)
      loss.backward()

      try:
        data_target,label_target=iteratore2.next()
      except StopIteration:
        iteratore2=iter(target_dataloader)
        data_target,label_target=iteratore2.next()

      images = data_target.to(DEVICE)
      
      #3RD STEP
      p = float(current_step + epoch * len(target_dataloader)) / NUM_EPOCHS / len(target_dataloader)
      ALFA = 2. / (1. + np.exp(-LAMBDA * p)) - 1
      outputs = nuova(images,ALFA)
      tensore=torch.tensor(np.ones((BATCH_SIZE,), dtype=int)).to(DEVICE)
      loss = criterion(outputs,tensore)
      loss.backward()
      
      optimizer.step()
      current_step += 1

    scheduler.step()

  return nuova

def testing_part(nuova,test_dataloader,test_dataset):
  nuova = nuova.to(DEVICE)
  nuova.train(False)

  running_corrects = 0
  for images, labels in tqdm(test_dataloader):
    images = images.to(DEVICE)
    labels = labels.to(DEVICE)

    outputs = nuova(images,None)

    _, preds = torch.max(outputs.data, 1)

    running_corrects += torch.sum(preds == labels.data).data.item()

  accuracy = running_corrects / float(len(test_dataset))

  return accuracy

#def create_model_with_adaptation(BATCH_SIZE,LR,NUM_EPOCHS,STEP_SIZE,GAMMA,SGD,ALFA):
def create_model_with_adaptation_LAMBDA(BATCH_SIZE,LR,NUM_EPOCHS,STEP_SIZE,GAMMA,SGD,LAMBDA):
  nuova,source_dataloder,target_dataloader,test_dataloader,criterion,optimizer,scheduler=preparation_part(photo_dataset,cartoon_dataset,BATCH_SIZE,NUM_CLASSES,LR,GAMMA,SGD,STEP_SIZE)
  #nuova=training_part(nuova,NUM_EPOCHS,BATCH_SIZE,ALFA,source_dataloder,target_dataloader,optimizer,criterion,scheduler)
  nuova=training_part(nuova,NUM_EPOCHS,BATCH_SIZE,LAMBDA,source_dataloder,target_dataloader,optimizer,criterion,scheduler)

  accuracy_1=testing_part(nuova,test_dataloader,cartoon_dataset)

  nuova,source_dataloder,target_dataloader,test_dataloader,criterion,optimizer,scheduler=preparation_part(photo_dataset,sketch_dataset,BATCH_SIZE,NUM_CLASSES,LR,GAMMA,SGD,STEP_SIZE)
  #nuova=training_part(nuova,NUM_EPOCHS,BATCH_SIZE,ALFA,source_dataloder,target_dataloader,optimizer,criterion,scheduler)
  nuova=training_part(nuova,NUM_EPOCHS,BATCH_SIZE,LAMBDA,source_dataloder,target_dataloader,optimizer,criterion,scheduler)
  accuracy_2=testing_part(nuova,test_dataloader,sketch_dataset)

  accuracy=(accuracy_1+accuracy_2)/2
  return accuracy

"""**ALFA FISSO**

*GRID PREPARATION*
"""

#definizione dei range
BATCH_SIZE_range=[128]
LR_range=[1e-3,5e-4,1e-4,5e-5]
NUM_EPOCHS_range=[30,40]
STEP_SIZE_range=[20]
GAMMA_range=[0.1,0.2]
SGD_range=[True,False]
ALFA_range=[0.1,0.5,0.8,1]

param_grid=dict(BATCH_SIZE=BATCH_SIZE_range,LR=LR_range,NUM_EPOCHS=NUM_EPOCHS_range,STEP_SIZE=STEP_SIZE_range,GAMMA=GAMMA_range,SGD=SGD_range,ALFA=ALFA_range)

best_acc=0
best_param=None
totale=len(list(ParameterGrid(param_grid)))
i=1
for combinazione in list(ParameterGrid(param_grid)):
  print(combinazione)
  print(f"Combination number {i} of {totale}")
  accuracy=create_model_with_adaptation(combinazione["BATCH_SIZE"],combinazione["LR"],combinazione["NUM_EPOCHS"],combinazione["STEP_SIZE"],combinazione["GAMMA"],combinazione["SGD"],combinazione["ALFA"])
  print(accuracy)
  if accuracy>best_acc:
    best_acc=accuracy
    best_param=combinazione
  i=i+1

"""*BEST FOUND*"""

best_param
'''
{'ALFA': 0.8,
 'BATCH_SIZE': 128,
 'GAMMA': 0.2,
 'LR': 0.0001,
 'NUM_EPOCHS': 30,
 'SGD': True,
 'STEP_SIZE': 20}

BATCH_SIZE = best_param["BATCH_SIZE"]
LR = best_param["LR"]            
NUM_EPOCHS = best_param["NUM_EPOCHS"] 
STEP_SIZE = best_param["STEP_SIZE"]
GAMMA = best_param["GAMMA"]
SGD=best_param["SGD"]
ALFA=best_param["ALFA"]

source_dataloader = DataLoader(photo_dataset, batch_size=BATCH_SIZE, shuffle=True,num_workers=4,drop_last=True)
target_dataloader = DataLoader(art_painting_dataset, batch_size=BATCH_SIZE, num_workers=4,shuffle=True,drop_last=True)
test_dataloader = DataLoader(art_painting_dataset, batch_size=BATCH_SIZE,num_workers=4, shuffle=False)
 
nuova=alexnet_nuova(pretrained=True)
nuova.DANN_classifier[1].weight.data=nuova.classifier[1].weight.data
nuova.DANN_classifier[1].bias.data=nuova.classifier[1].bias.data
nuova.DANN_classifier[4].weight.data=nuova.classifier[4].weight.data
nuova.DANN_classifier[4].bias.data=nuova.classifier[4].bias.data
nuova.DANN_classifier[6].weight.data=nuova.classifier[6].weight.data
nuova.DANN_classifier[6].bias.data=nuova.classifier[6].bias.data
nuova.classifier[6] = nn.Linear(4096, NUM_CLASSES)
nuova.DANN_classifier[6] = nn.Linear(4096, 2)

criterion = nn.CrossEntropyLoss() 
parameters_to_optimize = nuova.parameters() 
if SGD==True:
  optimizer = optim.SGD(parameters_to_optimize, lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)
else:
  optimizer = optim.Adam(parameters_to_optimize, lr=LR)
scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=STEP_SIZE, gamma=GAMMA)

"""*Train*"""

nuova = nuova.to(DEVICE) 
cudnn.benchmark

current_step = 0
for epoch in range(NUM_EPOCHS):
  print('Starting epoch {}/{}, LR = {}'.format(epoch+1, NUM_EPOCHS, scheduler.get_lr()))

  iteratore1=iter(source_dataloader)
  iteratore2=iter(target_dataloader)
  lunghezza=max(len(source_dataloader),len(target_dataloader))

  for i in range(lunghezza):
    print(i)
    try:
      data_source,label_source=iteratore1.next()
    except StopIteration:
      iteratore1=iter(source_dataloader)
      data_source,label_source=iteratore1.next()
    
    images = data_source.to(DEVICE)
    labels = label_source.to(DEVICE)

    nuova.train()
    optimizer.zero_grad()
  
    #1ST STEP
    outputs = nuova(images)
    loss = criterion(outputs, labels)
    loss.backward()
  
    #2ND STEP
    outputs = nuova(images,ALFA)
    tensore=torch.tensor(np.zeros((BATCH_SIZE,), dtype=int)).to(DEVICE)
    loss = criterion(outputs,tensore)
    loss.backward()
  
    try:
      data_target,label_target=iteratore2.next()
    except StopIteration:
      iteratore2=iter(target_dataloader)
      data_target,label_target=iteratore2.next()

    images = data_target.to(DEVICE)
  
    #3RD STEP
    outputs = nuova(images,ALFA)
    tensore=torch.tensor(np.ones((BATCH_SIZE,), dtype=int)).to(DEVICE)
    loss = criterion(outputs,tensore)
    loss.backward()
    
    optimizer.step() 
    current_step += 1

  scheduler.step()

"""*Test*"""

nuova = nuova.to(DEVICE)
nuova.train(False)

running_corrects = 0
for images, labels in tqdm(test_dataloader):
  images = images.to(DEVICE)
  labels = labels.to(DEVICE)

  outputs = nuova(images,None)

  _, preds = torch.max(outputs.data, 1)

  running_corrects += torch.sum(preds == labels.data).data.item()

accuracy = running_corrects / float(len(art_painting_dataset))

print('Test Accuracy: {}'.format(accuracy))

accuracy_totale=[]
for i in range (5):
  BATCH_SIZE = best_param["BATCH_SIZE"]
  LR = best_param["LR"]            
  NUM_EPOCHS = best_param["NUM_EPOCHS"] 
  STEP_SIZE = best_param["STEP_SIZE"]
  GAMMA = best_param["GAMMA"]
  SGD=best_param["SGD"]
  ALFA=best_param["ALFA"]

  source_dataloader = DataLoader(photo_dataset, batch_size=BATCH_SIZE, shuffle=True,num_workers=4,drop_last=True)
  target_dataloader = DataLoader(art_painting_dataset, batch_size=BATCH_SIZE, num_workers=4,shuffle=True,drop_last=True)
  test_dataloader = DataLoader(art_painting_dataset, batch_size=BATCH_SIZE,num_workers=4, shuffle=False)
  
  nuova=alexnet_nuova(pretrained=True)
  nuova.DANN_classifier[1].weight.data=nuova.classifier[1].weight.data
  nuova.DANN_classifier[1].bias.data=nuova.classifier[1].bias.data
  nuova.DANN_classifier[4].weight.data=nuova.classifier[4].weight.data
  nuova.DANN_classifier[4].bias.data=nuova.classifier[4].bias.data
  nuova.DANN_classifier[6].weight.data=nuova.classifier[6].weight.data
  nuova.DANN_classifier[6].bias.data=nuova.classifier[6].bias.data
  nuova.classifier[6] = nn.Linear(4096, NUM_CLASSES)
  nuova.DANN_classifier[6] = nn.Linear(4096, 2)

  criterion = nn.CrossEntropyLoss() 
  parameters_to_optimize = nuova.parameters() 
  if SGD==True:
    optimizer = optim.SGD(parameters_to_optimize, lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)
  else:
    optimizer = optim.Adam(parameters_to_optimize, lr=LR)
  scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=STEP_SIZE, gamma=GAMMA)

  nuova = nuova.to(DEVICE) 
  cudnn.benchmark

  current_step = 0
  for epoch in range(NUM_EPOCHS):
    print('Starting epoch {}/{}, LR = {}'.format(epoch+1, NUM_EPOCHS, scheduler.get_lr()))

    iteratore1=iter(source_dataloader)
    iteratore2=iter(target_dataloader)
    lunghezza=max(len(source_dataloader),len(target_dataloader))

    for i in range(lunghezza):
      print(i)
      try:
        data_source,label_source=iteratore1.next()
      except StopIteration:
        iteratore1=iter(source_dataloader)
        data_source,label_source=iteratore1.next()
      
      images = data_source.to(DEVICE)
      labels = label_source.to(DEVICE)

      nuova.train()
      optimizer.zero_grad()
    
      #1ST STEP
      outputs = nuova(images)
      loss = criterion(outputs, labels)
      loss.backward()
    
      #2ND STEP
      outputs = nuova(images,ALFA)
      tensore=torch.tensor(np.zeros((BATCH_SIZE,), dtype=int)).to(DEVICE)
      loss = criterion(outputs,tensore)
      loss.backward()
    
      try:
        data_target,label_target=iteratore2.next()
      except StopIteration:
        iteratore2=iter(target_dataloader)
        data_target,label_target=iteratore2.next()

      images = data_target.to(DEVICE)
    
      #3RD STEP
      outputs = nuova(images,ALFA)
      tensore=torch.tensor(np.ones((BATCH_SIZE,), dtype=int)).to(DEVICE)
      loss = criterion(outputs,tensore)
      loss.backward()
      
      optimizer.step() 
      current_step += 1

    scheduler.step() 

  nuova = nuova.to(DEVICE)
  nuova.train(False)

  running_corrects = 0
  for images, labels in tqdm(test_dataloader):
    images = images.to(DEVICE)
    labels = labels.to(DEVICE)

    outputs = nuova(images,None)

    _, preds = torch.max(outputs.data, 1)

    running_corrects += torch.sum(preds == labels.data).data.item()

  accuracy = running_corrects / float(len(art_painting_dataset))
  accuracy_totale.append(accuracy)
  print('Test Accuracy: {}'.format(accuracy))

acc=np.array(accuracy_totale)
print(f"ACCURACY: {acc.mean()} +- {acc.std()}")

"""**ALFA CHE VARIA**

*GRID PREPARATION*
"""

#definizione dei range
BATCH_SIZE_range=[128]
LR_range=[1e-3,5e-4,1e-4,5e-5]
NUM_EPOCHS_range=[30,40]
STEP_SIZE_range=[20]
GAMMA_range=[0.1,0.2]
SGD_range=[True]
LAMBDA_range=[2,5,10,15]

param_grid=dict(BATCH_SIZE=BATCH_SIZE_range,LR=LR_range,NUM_EPOCHS=NUM_EPOCHS_range,STEP_SIZE=STEP_SIZE_range,GAMMA=GAMMA_range,SGD=SGD_range,LAMBDA=LAMBDA_range)

best_acc=0
best_param=None
totale=len(list(ParameterGrid(param_grid)))
i=1
for combinazione in list(ParameterGrid(param_grid)):
  print(combinazione)
  print(f"Combination number {i} of {totale}")
  accuracy=create_model_with_adaptation_LAMBDA(combinazione["BATCH_SIZE"],combinazione["LR"],combinazione["NUM_EPOCHS"],combinazione["STEP_SIZE"],combinazione["GAMMA"],combinazione["SGD"],combinazione["LAMBDA"])
  print(accuracy)
  if accuracy>best_acc:
    best_acc=accuracy
    best_param=combinazione
  i=i+1

"""*BEST FOUND*"""

best_param
'''
{'BATCH_SIZE': 128, 'GAMMA': 0.2, 'LAMBDA': 10, 'LR': 0.0001, 'NUM_EPOCHS': 30, 'SGD': True, 'STEP_SIZE': 20}

BATCH_SIZE = best_param["BATCH_SIZE"]
LR = best_param["LR"]            
NUM_EPOCHS = best_param["NUM_EPOCHS"] 
STEP_SIZE = best_param["STEP_SIZE"]
GAMMA = best_param["GAMMA"]
SGD=best_param["SGD"]
LAMBDA=best_param["LAMBDA"]

source_dataloader = DataLoader(photo_dataset, batch_size=BATCH_SIZE, shuffle=True,num_workers=4,drop_last=True)
target_dataloader = DataLoader(art_painting_dataset, batch_size=BATCH_SIZE, num_workers=4,shuffle=True,drop_last=True)
test_dataloader = DataLoader(art_painting_dataset, batch_size=BATCH_SIZE,num_workers=4, shuffle=False)
 
nuova=alexnet_nuova(pretrained=True)
nuova.DANN_classifier[1].weight.data=nuova.classifier[1].weight.data
nuova.DANN_classifier[1].bias.data=nuova.classifier[1].bias.data
nuova.DANN_classifier[4].weight.data=nuova.classifier[4].weight.data
nuova.DANN_classifier[4].bias.data=nuova.classifier[4].bias.data
nuova.DANN_classifier[6].weight.data=nuova.classifier[6].weight.data
nuova.DANN_classifier[6].bias.data=nuova.classifier[6].bias.data
nuova.classifier[6] = nn.Linear(4096, NUM_CLASSES)
nuova.DANN_classifier[6] = nn.Linear(4096, 2)

criterion = nn.CrossEntropyLoss() 
parameters_to_optimize = nuova.parameters() 
if SGD==True:
  optimizer = optim.SGD(parameters_to_optimize, lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)
else:
  optimizer = optim.Adam(parameters_to_optimize, lr=LR)
scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=STEP_SIZE, gamma=GAMMA)

"""*Train*"""

nuova = nuova.to(DEVICE) 
cudnn.benchmark

list_loss_source_clas=[]
list_loss_source_domain=[]
list_loss_target_domain=[]
current_step = 0
for epoch in range(NUM_EPOCHS):
  print('Starting epoch {}/{}, LR = {}'.format(epoch+1, NUM_EPOCHS, scheduler.get_lr()))

  iteratore1=iter(source_dataloader)
  iteratore2=iter(target_dataloader)
  lunghezza=max(len(source_dataloader),len(target_dataloader))

  loss_source_class=0
  loss_source_domain=0
  loss_target_domain=0
  conto_source_class=0
  conto_source_domain=0
  conto_target_domain=0

  for i in range(lunghezza):
    print(i)
    try:
      data_source,label_source=iteratore1.next()
    except StopIteration:
      iteratore1=iter(source_dataloader)
      data_source,label_source=iteratore1.next()
    
    images = data_source.to(DEVICE)
    labels = label_source.to(DEVICE)

    nuova.train()
    optimizer.zero_grad()
  
    #1ST STEP
    outputs = nuova(images)
    loss = criterion(outputs, labels)
    loss_source_class=loss_source_class+loss.item()
    conto_source_class=conto_source_class+1
    loss.backward()
  
    #2ND STEP
    p = float(current_step + epoch * len(source_dataloader)) / NUM_EPOCHS / len(source_dataloader)
    ALFA = 2. / (1. + np.exp(-LAMBDA * p)) - 1
    print(ALFA)
    outputs = nuova(images,ALFA)
    tensore=torch.tensor(np.zeros((BATCH_SIZE,), dtype=int)).to(DEVICE)
    loss = criterion(outputs,tensore)
    loss_source_domain=loss_source_domain+loss.item()
    conto_source_domain=conto_source_domain+1
    loss.backward()
  
    try:
      data_target,label_target=iteratore2.next()
    except StopIteration:
      iteratore2=iter(target_dataloader)
      data_target,label_target=iteratore2.next()

    images = data_target.to(DEVICE)
  
    #3RD STEP
    p = float(current_step + epoch * len(target_dataloader)) / NUM_EPOCHS / len(target_dataloader)
    ALFA = 2. / (1. + np.exp(-LAMBDA * p)) - 1
    print(ALFA)
    outputs = nuova(images,ALFA)
    tensore=torch.tensor(np.ones((BATCH_SIZE,), dtype=int)).to(DEVICE)
    loss = criterion(outputs,tensore)
    loss_target_domain=loss_target_domain+loss.item()
    conto_target_domain=conto_target_domain+1
    loss.backward()
    
    optimizer.step() 
    current_step += 1
  
  media_source_class=loss_source_class/conto_source_class
  media_source_domain=loss_source_domain/conto_source_domain
  media_target_domain=loss_target_domain/conto_target_domain
  list_loss_source_clas.append(media_source_class)
  list_loss_source_domain.append(media_source_domain)
  list_loss_target_domain.append(media_target_domain)

  scheduler.step()

"""*Test*"""

nuova = nuova.to(DEVICE)
nuova.train(False)

running_corrects = 0
for images, labels in tqdm(test_dataloader):
  images = images.to(DEVICE)
  labels = labels.to(DEVICE)

  outputs = nuova(images,None)

  _, preds = torch.max(outputs.data, 1)

  running_corrects += torch.sum(preds == labels.data).data.item()

accuracy = running_corrects / float(len(art_painting_dataset))

print('Test Accuracy: {}'.format(accuracy))

plt.figure(figsize=(10,4))

plt.plot(range(1,31),list_loss_source_clas, label ="Source classifier")
plt.ylabel("Loss")
plt.xlabel("Epochs")
plt.xticks(range(1,31))
plt.ylim(0,2)
plt.legend()
plt.title("LOSS CLASSIFIER")
plt.savefig("LOSS CLASSIFIER")
plt.show()

plt.figure(figsize=(10,4))

plt.plot(range(1,31),list_loss_source_domain, label ="Source domain")
plt.plot(range(1,31),list_loss_target_domain, label ="Target domain")

plt.ylabel("Loss")
plt.xlabel("Epochs")
plt.xticks(range(1,31))
plt.ylim(0,1)
plt.legend()
plt.title("LOSS DISCRIMINATOR")
plt.savefig("LOSS DISCRIMINATOR")
plt.show()

accuracy_totale=[]
for k in range (5):
  print(k)
  BATCH_SIZE = best_param["BATCH_SIZE"]
  LR = best_param["LR"]            
  NUM_EPOCHS = best_param["NUM_EPOCHS"] 
  STEP_SIZE = best_param["STEP_SIZE"]
  GAMMA = best_param["GAMMA"]
  SGD=best_param["SGD"]
  LAMBDA=best_param["LAMBDA"]

  source_dataloader = DataLoader(photo_dataset, batch_size=BATCH_SIZE, shuffle=True,num_workers=4,drop_last=True)
  target_dataloader = DataLoader(art_painting_dataset, batch_size=BATCH_SIZE, num_workers=4,shuffle=True,drop_last=True)
  test_dataloader = DataLoader(art_painting_dataset, batch_size=BATCH_SIZE,num_workers=4, shuffle=False)
  
  nuova=alexnet_nuova(pretrained=True)
  nuova.DANN_classifier[1].weight.data=nuova.classifier[1].weight.data
  nuova.DANN_classifier[1].bias.data=nuova.classifier[1].bias.data
  nuova.DANN_classifier[4].weight.data=nuova.classifier[4].weight.data
  nuova.DANN_classifier[4].bias.data=nuova.classifier[4].bias.data
  nuova.DANN_classifier[6].weight.data=nuova.classifier[6].weight.data
  nuova.DANN_classifier[6].bias.data=nuova.classifier[6].bias.data
  nuova.classifier[6] = nn.Linear(4096, NUM_CLASSES)
  nuova.DANN_classifier[6] = nn.Linear(4096, 2)

  criterion = nn.CrossEntropyLoss() 
  parameters_to_optimize = nuova.parameters() 
  if SGD==True:
    optimizer = optim.SGD(parameters_to_optimize, lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)
  else:
    optimizer = optim.Adam(parameters_to_optimize, lr=LR)
  scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=STEP_SIZE, gamma=GAMMA)
  nuova = nuova.to(DEVICE) 
  cudnn.benchmark

  list_loss_source_clas=[]
  list_loss_source_domain=[]
  list_loss_target_domain=[]
  current_step = 0
  for epoch in range(NUM_EPOCHS):
    print('Starting epoch {}/{}, LR = {}'.format(epoch+1, NUM_EPOCHS, scheduler.get_lr()))

    iteratore1=iter(source_dataloader)
    iteratore2=iter(target_dataloader)
    lunghezza=max(len(source_dataloader),len(target_dataloader))

    loss_source_class=0
    loss_source_domain=0
    loss_target_domain=0
    conto_source_class=0
    conto_source_domain=0
    conto_target_domain=0

    for i in range(lunghezza):
      try:
        data_source,label_source=iteratore1.next()
      except StopIteration:
        iteratore1=iter(source_dataloader)
        data_source,label_source=iteratore1.next()
      
      images = data_source.to(DEVICE)
      labels = label_source.to(DEVICE)

      nuova.train()
      optimizer.zero_grad()
    
      #1ST STEP
      outputs = nuova(images)
      loss = criterion(outputs, labels)
      loss_source_class=loss_source_class+loss.item()
      conto_source_class=conto_source_class+1
      loss.backward()
    
      #2ND STEP
      p = float(current_step + epoch * len(source_dataloader)) / NUM_EPOCHS / len(source_dataloader)
      ALFA = 2. / (1. + np.exp(-LAMBDA * p)) - 1
      outputs = nuova(images,ALFA)
      tensore=torch.tensor(np.zeros((BATCH_SIZE,), dtype=int)).to(DEVICE)
      loss = criterion(outputs,tensore)
      loss_source_domain=loss_source_domain+loss.item()
      conto_source_domain=conto_source_domain+1
      loss.backward()
    
      try:
        data_target,label_target=iteratore2.next()
      except StopIteration:
        iteratore2=iter(target_dataloader)
        data_target,label_target=iteratore2.next()

      images = data_target.to(DEVICE)
    
      #3RD STEP
      p = float(current_step + epoch * len(target_dataloader)) / NUM_EPOCHS / len(target_dataloader)
      ALFA = 2. / (1. + np.exp(-LAMBDA * p)) - 1
      outputs = nuova(images,ALFA)
      tensore=torch.tensor(np.ones((BATCH_SIZE,), dtype=int)).to(DEVICE)
      loss = criterion(outputs,tensore)
      loss_target_domain=loss_target_domain+loss.item()
      conto_target_domain=conto_target_domain+1
      loss.backward()
      
      optimizer.step() 
      current_step += 1
    
    media_source_class=loss_source_class/conto_source_class
    media_source_domain=loss_source_domain/conto_source_domain
    media_target_domain=loss_target_domain/conto_target_domain
    list_loss_source_clas.append(media_source_class)
    list_loss_source_domain.append(media_source_domain)
    list_loss_target_domain.append(media_target_domain)

    scheduler.step() 
  
  nuova = nuova.to(DEVICE)
  nuova.train(False)

  running_corrects = 0
  for images, labels in tqdm(test_dataloader):
    images = images.to(DEVICE)
    labels = labels.to(DEVICE)

    outputs = nuova(images,None)

    _, preds = torch.max(outputs.data, 1)

    running_corrects += torch.sum(preds == labels.data).data.item()

  accuracy = running_corrects / float(len(art_painting_dataset))
  accuracy_totale.append(accuracy)
  print('Test Accuracy: {}'.format(accuracy))

acc=np.array(accuracy_totale)
print(f"ACCURACY: {acc.mean()} +- {acc.std()}")

"""**TRAIN SU PHOTO, TEST SU CARTOON**

*WITHOUT ADAPTATION*
"""

BATCH_SIZE = 256
LR = 1e-3           
NUM_EPOCHS = 30
STEP_SIZE = 20
GAMMA = 0.1
SGD=True

train_dataloader = DataLoader(photo_dataset, batch_size=BATCH_SIZE, shuffle=True,num_workers=4,drop_last=True)
test_dataloader_art = DataLoader(cartoon_dataset, batch_size=BATCH_SIZE, num_workers=4,shuffle=False)
 
net = torchvision.models.alexnet(pretrained=True)
net.classifier[6] = nn.Linear(4096, NUM_CLASSES)

criterion = nn.CrossEntropyLoss() 
parameters_to_optimize = net.parameters() 
if SGD==True:
  optimizer = optim.SGD(parameters_to_optimize, lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)
else:
  optimizer = optim.Adam(parameters_to_optimize, lr=LR)
scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=STEP_SIZE, gamma=GAMMA)

net = net.to(DEVICE) 
cudnn.benchmark

current_step = 0
for epoch in range(NUM_EPOCHS):
  print('Starting epoch {}/{}, LR = {}'.format(epoch+1, NUM_EPOCHS, scheduler.get_lr()))

  for images, labels in train_dataloader:
    images = images.to(DEVICE)
    labels = labels.to(DEVICE)

    net.train()
    optimizer.zero_grad()

    # Forward pass to the network
    outputs = net(images)

    loss = criterion(outputs, labels)

    # Compute gradients for each layer and update weights
    loss.backward()  # backward pass: computes gradients
    optimizer.step() # update weights based on accumulated gradients

    current_step += 1

  # Step the scheduler
  scheduler.step()

net = net.to(DEVICE)
net.train(False)

running_corrects = 0
for images, labels in tqdm(test_dataloader_art):
  images = images.to(DEVICE)
  labels = labels.to(DEVICE)

  outputs = net(images)

  _, preds = torch.max(outputs.data, 1)

  running_corrects += torch.sum(preds == labels.data).data.item()

accuracy = running_corrects / float(len(art_painting_dataset))

print('Test Accuracy: {}'.format(accuracy))

"""*WITH ADAPTATION - alfa fisso*"""

BATCH_SIZE = 128
LR = 1e-4            
NUM_EPOCHS = 30 
STEP_SIZE = 20
GAMMA = 0.2
SGD=True
ALFA=0.8

source_dataloader = DataLoader(photo_dataset, batch_size=BATCH_SIZE, shuffle=True,num_workers=4,drop_last=True)
target_dataloader = DataLoader(cartoon_dataset, batch_size=BATCH_SIZE, num_workers=4,shuffle=True,drop_last=True)
test_dataloader = DataLoader(cartoon_dataset, batch_size=BATCH_SIZE,num_workers=4, shuffle=False)
 
nuova=alexnet_nuova(pretrained=True)
nuova.DANN_classifier[1].weight.data=nuova.classifier[1].weight.data
nuova.DANN_classifier[1].bias.data=nuova.classifier[1].bias.data
nuova.DANN_classifier[4].weight.data=nuova.classifier[4].weight.data
nuova.DANN_classifier[4].bias.data=nuova.classifier[4].bias.data
nuova.DANN_classifier[6].weight.data=nuova.classifier[6].weight.data
nuova.DANN_classifier[6].bias.data=nuova.classifier[6].bias.data
nuova.classifier[6] = nn.Linear(4096, NUM_CLASSES)
nuova.DANN_classifier[6] = nn.Linear(4096, 2)

criterion = nn.CrossEntropyLoss() 
parameters_to_optimize = nuova.parameters() 
if SGD==True:
  optimizer = optim.SGD(parameters_to_optimize, lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)
else:
  optimizer = optim.Adam(parameters_to_optimize, lr=LR)
scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=STEP_SIZE, gamma=GAMMA)

nuova = nuova.to(DEVICE) 
cudnn.benchmark

current_step = 0
for epoch in range(NUM_EPOCHS):
  print('Starting epoch {}/{}, LR = {}'.format(epoch+1, NUM_EPOCHS, scheduler.get_lr()))

  iteratore1=iter(source_dataloader)
  iteratore2=iter(target_dataloader)
  lunghezza=max(len(source_dataloader),len(target_dataloader))

  for i in range(lunghezza):
    print(i)
    try:
      data_source,label_source=iteratore1.next()
    except StopIteration:
      iteratore1=iter(source_dataloader)
      data_source,label_source=iteratore1.next()
    
    images = data_source.to(DEVICE)
    labels = label_source.to(DEVICE)

    nuova.train()
    optimizer.zero_grad()
  
    #1ST STEP
    outputs = nuova(images)
    loss = criterion(outputs, labels)
    loss.backward()
  
    #2ND STEP
    outputs = nuova(images,ALFA)
    tensore=torch.tensor(np.zeros((BATCH_SIZE,), dtype=int)).to(DEVICE)
    loss = criterion(outputs,tensore)
    loss.backward()
  
    try:
      data_target,label_target=iteratore2.next()
    except StopIteration:
      iteratore2=iter(target_dataloader)
      data_target,label_target=iteratore2.next()

    images = data_target.to(DEVICE)
  
    #3RD STEP
    outputs = nuova(images,ALFA)
    tensore=torch.tensor(np.ones((BATCH_SIZE,), dtype=int)).to(DEVICE)
    loss = criterion(outputs,tensore)
    loss.backward()
    
    optimizer.step() 
    current_step += 1

  scheduler.step()

nuova = nuova.to(DEVICE)
nuova.train(False)

running_corrects = 0
for images, labels in tqdm(test_dataloader):
  images = images.to(DEVICE)
  labels = labels.to(DEVICE)

  outputs = nuova(images,None)

  _, preds = torch.max(outputs.data, 1)

  running_corrects += torch.sum(preds == labels.data).data.item()

accuracy = running_corrects / float(len(art_painting_dataset))

print('Test Accuracy: {}'.format(accuracy))

"""*WITH ADAPTATION - ALFA DINAMICO*"""

BATCH_SIZE = 128
LR = 1e-4            
NUM_EPOCHS = 30 
STEP_SIZE = 20
GAMMA = 0.2
SGD=True
LAMBDA=10

source_dataloader = DataLoader(photo_dataset, batch_size=BATCH_SIZE, shuffle=True,num_workers=4,drop_last=True)
target_dataloader = DataLoader(cartoon_dataset, batch_size=BATCH_SIZE, num_workers=4,shuffle=True,drop_last=True)
test_dataloader = DataLoader(cartoon_dataset, batch_size=BATCH_SIZE,num_workers=4, shuffle=False)
 
nuova=alexnet_nuova(pretrained=True)
nuova.DANN_classifier[1].weight.data=nuova.classifier[1].weight.data
nuova.DANN_classifier[1].bias.data=nuova.classifier[1].bias.data
nuova.DANN_classifier[4].weight.data=nuova.classifier[4].weight.data
nuova.DANN_classifier[4].bias.data=nuova.classifier[4].bias.data
nuova.DANN_classifier[6].weight.data=nuova.classifier[6].weight.data
nuova.DANN_classifier[6].bias.data=nuova.classifier[6].bias.data
nuova.classifier[6] = nn.Linear(4096, NUM_CLASSES)
nuova.DANN_classifier[6] = nn.Linear(4096, 2)

criterion = nn.CrossEntropyLoss() 
parameters_to_optimize = nuova.parameters() 
if SGD==True:
  optimizer = optim.SGD(parameters_to_optimize, lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)
else:
  optimizer = optim.Adam(parameters_to_optimize, lr=LR)
scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=STEP_SIZE, gamma=GAMMA)

nuova = nuova.to(DEVICE) 
cudnn.benchmark

current_step = 0
for epoch in range(NUM_EPOCHS):
  print('Starting epoch {}/{}, LR = {}'.format(epoch+1, NUM_EPOCHS, scheduler.get_lr()))

  iteratore1=iter(source_dataloader)
  iteratore2=iter(target_dataloader)
  lunghezza=max(len(source_dataloader),len(target_dataloader))

  for i in range(lunghezza):
    print(i)
    try:
      data_source,label_source=iteratore1.next()
    except StopIteration:
      iteratore1=iter(source_dataloader)
      data_source,label_source=iteratore1.next()
    
    images = data_source.to(DEVICE)
    labels = label_source.to(DEVICE)

    nuova.train()
    optimizer.zero_grad()
  
    #1ST STEP
    outputs = nuova(images)
    loss = criterion(outputs, labels)
    loss.backward()
  
    #2ND STEP
    p = float(current_step + epoch * len(source_dataloader)) / NUM_EPOCHS / len(source_dataloader)
    ALFA = 2. / (1. + np.exp(-LAMBDA * p)) - 1
    print(ALFA)
    outputs = nuova(images,ALFA)
    tensore=torch.tensor(np.zeros((BATCH_SIZE,), dtype=int)).to(DEVICE)
    loss = criterion(outputs,tensore)
    loss.backward()
  
    try:
      data_target,label_target=iteratore2.next()
    except StopIteration:
      iteratore2=iter(target_dataloader)
      data_target,label_target=iteratore2.next()

    images = data_target.to(DEVICE)
  
    #3RD STEP
    p = float(current_step + epoch * len(target_dataloader)) / NUM_EPOCHS / len(target_dataloader)
    ALFA = 2. / (1. + np.exp(-LAMBDA * p)) - 1
    print(ALFA)
    outputs = nuova(images,ALFA)
    tensore=torch.tensor(np.ones((BATCH_SIZE,), dtype=int)).to(DEVICE)
    loss = criterion(outputs,tensore)
    loss.backward()
    
    optimizer.step() 
    current_step += 1
    
  scheduler.step()

"""*Test*"""

nuova = nuova.to(DEVICE)
nuova.train(False)

running_corrects = 0
for images, labels in tqdm(test_dataloader):
  images = images.to(DEVICE)
  labels = labels.to(DEVICE)

  outputs = nuova(images,None)

  _, preds = torch.max(outputs.data, 1)

  running_corrects += torch.sum(preds == labels.data).data.item()

accuracy = running_corrects / float(len(art_painting_dataset))

print('Test Accuracy: {}'.format(accuracy))